{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b453d9bc",
   "metadata": {},
   "source": [
    "### Get metadata: Use get_metadata function to crawl metadata by specified date range, the date is set from 1 January 2024 to 30 June 2024.\n",
    "### Download PDF files: use download_decisions function to download PDF files according to the metadata and save them to the specified directory.\n",
    "### The official website has a total of 3212 pieces of data, of which 1534 are upheld, not upheld 1678."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615523df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typer in d:\\py\\anaconda3\\lib\\site-packages (0.12.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\py\\anaconda3\\lib\\site-packages (from typer) (4.12.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\py\\anaconda3\\lib\\site-packages (from typer) (13.7.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\py\\anaconda3\\lib\\site-packages (from typer) (1.5.4)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\py\\anaconda3\\lib\\site-packages (from typer) (8.0.3)\n",
      "Requirement already satisfied: colorama in d:\\py\\anaconda3\\lib\\site-packages (from click>=8.0.0->typer) (0.4.4)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\py\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\py\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\py\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install typer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b57af6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 10 entries from page 0\n",
      "Scraping 10 entries from page 10\n",
      "Scraping 10 entries from page 20\n",
      "Scraping 10 entries from page 30\n",
      "Scraping 10 entries from page 40\n",
      "Scraping 10 entries from page 50\n",
      "Scraping 10 entries from page 60\n",
      "Scraping 10 entries from page 70\n",
      "Scraping 10 entries from page 80\n",
      "Scraping 10 entries from page 90\n",
      "Scraping 10 entries from page 100\n",
      "Scraping 10 entries from page 110\n",
      "Scraping 10 entries from page 120\n",
      "Scraping 10 entries from page 130\n",
      "Scraping 10 entries from page 140\n",
      "Scraping 10 entries from page 150\n",
      "Scraping 10 entries from page 160\n",
      "Scraping 10 entries from page 170\n",
      "Scraping 10 entries from page 180\n",
      "Scraping 10 entries from page 190\n",
      "Scraping 10 entries from page 200\n",
      "Scraping 10 entries from page 210\n",
      "Scraping 10 entries from page 220\n",
      "Scraping 10 entries from page 230\n",
      "Scraping 10 entries from page 240\n",
      "Scraping 10 entries from page 250\n",
      "Scraping 10 entries from page 260\n",
      "Scraping 10 entries from page 270\n",
      "Scraping 10 entries from page 280\n",
      "Scraping 10 entries from page 290\n",
      "Scraping 10 entries from page 300\n",
      "Scraping 10 entries from page 310\n",
      "Scraping 10 entries from page 320\n",
      "Scraping 10 entries from page 330\n",
      "Scraping 10 entries from page 340\n",
      "Scraping 10 entries from page 350\n",
      "Scraping 10 entries from page 360\n",
      "Scraping 10 entries from page 370\n",
      "Scraping 10 entries from page 380\n",
      "Scraping 10 entries from page 390\n",
      "Scraping 10 entries from page 400\n",
      "Scraping 10 entries from page 410\n",
      "Scraping 10 entries from page 420\n",
      "Scraping 10 entries from page 430\n",
      "Scraping 10 entries from page 440\n",
      "Scraping 10 entries from page 450\n",
      "Scraping 10 entries from page 460\n",
      "Scraping 10 entries from page 470\n",
      "Scraping 10 entries from page 480\n",
      "Scraping 10 entries from page 490\n",
      "Scraping 10 entries from page 500\n",
      "Scraping 10 entries from page 510\n",
      "Scraping 10 entries from page 520\n",
      "Scraping 10 entries from page 530\n",
      "Scraping 10 entries from page 540\n",
      "Scraping 10 entries from page 550\n",
      "Scraping 10 entries from page 560\n",
      "Scraping 10 entries from page 570\n",
      "Scraping 10 entries from page 580\n",
      "Scraping 10 entries from page 590\n",
      "Scraping 10 entries from page 600\n",
      "Scraping 10 entries from page 610\n",
      "Scraping 10 entries from page 620\n",
      "Scraping 10 entries from page 630\n",
      "Scraping 10 entries from page 640\n",
      "Scraping 10 entries from page 650\n",
      "Scraping 10 entries from page 660\n",
      "Scraping 10 entries from page 670\n",
      "Scraping 10 entries from page 680\n",
      "Scraping 10 entries from page 690\n",
      "Scraping 10 entries from page 700\n",
      "Scraping 10 entries from page 710\n",
      "Scraping 10 entries from page 720\n",
      "Scraping 10 entries from page 730\n",
      "Scraping 10 entries from page 740\n",
      "Scraping 10 entries from page 750\n",
      "Scraping 10 entries from page 760\n",
      "Scraping 10 entries from page 770\n",
      "Scraping 10 entries from page 780\n",
      "Scraping 10 entries from page 790\n",
      "Scraping 10 entries from page 800\n",
      "Scraping 10 entries from page 810\n",
      "Scraping 10 entries from page 820\n",
      "Scraping 10 entries from page 830\n",
      "Scraping 10 entries from page 840\n",
      "Scraping 10 entries from page 850\n",
      "Scraping 10 entries from page 860\n",
      "Scraping 10 entries from page 870\n",
      "Scraping 10 entries from page 880\n",
      "Scraping 10 entries from page 890\n",
      "Scraping 10 entries from page 900\n",
      "Scraping 10 entries from page 910\n",
      "Scraping 10 entries from page 920\n",
      "Scraping 10 entries from page 930\n",
      "Scraping 10 entries from page 940\n",
      "Scraping 10 entries from page 950\n",
      "Scraping 10 entries from page 960\n",
      "Scraping 10 entries from page 970\n",
      "Scraping 10 entries from page 980\n",
      "Scraping 10 entries from page 990\n",
      "Scraping 10 entries from page 1000\n",
      "Scraping 10 entries from page 1010\n",
      "Scraping 10 entries from page 1020\n",
      "Scraping 10 entries from page 1030\n",
      "Scraping 10 entries from page 1040\n",
      "Scraping 10 entries from page 1050\n",
      "Scraping 10 entries from page 1060\n",
      "Scraping 10 entries from page 1070\n",
      "Scraping 10 entries from page 1080\n",
      "Scraping 10 entries from page 1090\n",
      "Scraping 10 entries from page 1100\n",
      "Scraping 10 entries from page 1110\n",
      "Scraping 10 entries from page 1120\n",
      "Scraping 10 entries from page 1130\n",
      "Scraping 10 entries from page 1140\n",
      "Scraping 10 entries from page 1150\n",
      "Scraping 10 entries from page 1160\n",
      "Scraping 10 entries from page 1170\n",
      "Scraping 10 entries from page 1180\n",
      "Scraping 10 entries from page 1190\n",
      "Scraping 10 entries from page 1200\n",
      "Scraping 10 entries from page 1210\n",
      "Scraping 10 entries from page 1220\n",
      "Scraping 10 entries from page 1230\n",
      "Scraping 10 entries from page 1240\n",
      "Scraping 10 entries from page 1250\n",
      "Scraping 10 entries from page 1260\n",
      "Scraping 10 entries from page 1270\n",
      "Scraping 10 entries from page 1280\n",
      "Scraping 10 entries from page 1290\n",
      "Scraping 10 entries from page 1300\n",
      "Scraping 10 entries from page 1310\n",
      "Scraping 10 entries from page 1320\n",
      "Scraping 10 entries from page 1330\n",
      "Scraping 10 entries from page 1340\n",
      "Scraping 10 entries from page 1350\n",
      "Scraping 10 entries from page 1360\n",
      "Scraping 10 entries from page 1370\n",
      "Scraping 10 entries from page 1380\n",
      "Scraping 10 entries from page 1390\n",
      "Scraping 10 entries from page 1400\n",
      "Scraping 10 entries from page 1410\n",
      "Scraping 10 entries from page 1420\n",
      "Scraping 10 entries from page 1430\n",
      "Scraping 10 entries from page 1440\n",
      "Scraping 10 entries from page 1450\n",
      "Scraping 10 entries from page 1460\n",
      "Scraping 10 entries from page 1470\n",
      "Scraping 10 entries from page 1480\n",
      "Scraping 10 entries from page 1490\n",
      "Scraping 10 entries from page 1500\n",
      "Scraping 10 entries from page 1510\n",
      "Scraping 10 entries from page 1520\n",
      "Scraping 10 entries from page 1530\n",
      "Scraping 10 entries from page 1540\n",
      "Scraping 10 entries from page 1550\n",
      "Scraping 10 entries from page 1560\n",
      "Scraping 10 entries from page 1570\n",
      "Scraping 10 entries from page 1580\n",
      "Scraping 10 entries from page 1590\n",
      "Scraping 10 entries from page 1600\n",
      "Scraping 10 entries from page 1610\n",
      "Scraping 10 entries from page 1620\n",
      "Scraping 10 entries from page 1630\n",
      "Scraping 10 entries from page 1640\n",
      "Scraping 10 entries from page 1650\n",
      "Scraping 10 entries from page 1660\n",
      "Scraping 10 entries from page 1670\n",
      "Scraping 10 entries from page 1680\n",
      "Scraping 10 entries from page 1690\n",
      "Scraping 10 entries from page 1700\n",
      "Scraping 10 entries from page 1710\n",
      "Scraping 10 entries from page 1720\n",
      "Scraping 10 entries from page 1730\n",
      "Scraping 10 entries from page 1740\n",
      "Scraping 10 entries from page 1750\n",
      "Scraping 10 entries from page 1760\n",
      "Scraping 10 entries from page 1770\n",
      "Scraping 10 entries from page 1780\n",
      "Scraping 10 entries from page 1790\n",
      "Scraping 10 entries from page 1800\n",
      "Scraping 10 entries from page 1810\n",
      "Scraping 10 entries from page 1820\n",
      "Scraping 10 entries from page 1830\n",
      "Scraping 10 entries from page 1840\n",
      "Scraping 10 entries from page 1850\n",
      "Scraping 10 entries from page 1860\n",
      "Scraping 10 entries from page 1870\n",
      "Scraping 10 entries from page 1880\n",
      "Scraping 10 entries from page 1890\n",
      "Scraping 10 entries from page 1900\n",
      "Scraping 10 entries from page 1910\n",
      "Scraping 10 entries from page 1920\n",
      "Scraping 10 entries from page 1930\n",
      "Scraping 10 entries from page 1940\n",
      "Scraping 10 entries from page 1950\n",
      "Scraping 10 entries from page 1960\n",
      "Scraping 10 entries from page 1970\n",
      "Scraping 10 entries from page 1980\n",
      "Scraping 10 entries from page 1990\n",
      "Scraping 10 entries from page 2000\n",
      "Scraping 10 entries from page 2010\n",
      "Scraping 10 entries from page 2020\n",
      "Scraping 10 entries from page 2030\n",
      "Scraping 10 entries from page 2040\n",
      "Scraping 10 entries from page 2050\n",
      "Scraping 10 entries from page 2060\n",
      "Scraping 10 entries from page 2070\n",
      "Scraping 10 entries from page 2080\n",
      "Scraping 10 entries from page 2090\n",
      "Scraping 10 entries from page 2100\n",
      "Scraping 10 entries from page 2110\n",
      "Scraping 10 entries from page 2120\n",
      "Scraping 10 entries from page 2130\n",
      "Scraping 10 entries from page 2140\n",
      "Scraping 10 entries from page 2150\n",
      "Scraping 10 entries from page 2160\n",
      "Scraping 10 entries from page 2170\n",
      "Scraping 10 entries from page 2180\n",
      "Scraping 10 entries from page 2190\n",
      "Scraping 10 entries from page 2200\n",
      "Scraping 10 entries from page 2210\n",
      "Scraping 10 entries from page 2220\n",
      "Scraping 10 entries from page 2230\n",
      "Scraping 10 entries from page 2240\n",
      "Scraping 10 entries from page 2250\n",
      "Scraping 10 entries from page 2260\n",
      "Scraping 10 entries from page 2270\n",
      "Scraping 10 entries from page 2280\n",
      "Scraping 10 entries from page 2290\n",
      "Scraping 10 entries from page 2300\n",
      "Scraping 10 entries from page 2310\n",
      "Scraping 10 entries from page 2320\n",
      "Scraping 10 entries from page 2330\n",
      "Scraping 10 entries from page 2340\n",
      "Scraping 10 entries from page 2350\n",
      "Scraping 10 entries from page 2360\n",
      "Scraping 10 entries from page 2370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 10 entries from page 2380\n",
      "Scraping 10 entries from page 2390\n",
      "Scraping 10 entries from page 2400\n",
      "Scraping 10 entries from page 2410\n",
      "Scraping 10 entries from page 2420\n",
      "Scraping 10 entries from page 2430\n",
      "Scraping 10 entries from page 2440\n",
      "Scraping 10 entries from page 2450\n",
      "Scraping 10 entries from page 2460\n",
      "Scraping 10 entries from page 2470\n",
      "Scraping 10 entries from page 2480\n",
      "Scraping 10 entries from page 2490\n",
      "Scraping 10 entries from page 2500\n",
      "Scraping 10 entries from page 2510\n",
      "Scraping 10 entries from page 2520\n",
      "Scraping 10 entries from page 2530\n",
      "Scraping 10 entries from page 2540\n",
      "Scraping 10 entries from page 2550\n",
      "Scraping 10 entries from page 2560\n",
      "Scraping 10 entries from page 2570\n",
      "Scraping 10 entries from page 2580\n",
      "Scraping 10 entries from page 2590\n",
      "Scraping 10 entries from page 2600\n",
      "Scraping 10 entries from page 2610\n",
      "Scraping 10 entries from page 2620\n",
      "Scraping 10 entries from page 2630\n",
      "Scraping 10 entries from page 2640\n",
      "Scraping 10 entries from page 2650\n",
      "Scraping 10 entries from page 2660\n",
      "Scraping 10 entries from page 2670\n",
      "Scraping 10 entries from page 2680\n",
      "Scraping 10 entries from page 2690\n",
      "Scraping 10 entries from page 2700\n",
      "Scraping 10 entries from page 2710\n",
      "Scraping 10 entries from page 2720\n",
      "Scraping 10 entries from page 2730\n",
      "Scraping 10 entries from page 2740\n",
      "Scraping 10 entries from page 2750\n",
      "Scraping 10 entries from page 2760\n",
      "Scraping 10 entries from page 2770\n",
      "Scraping 10 entries from page 2780\n",
      "Scraping 10 entries from page 2790\n",
      "Scraping 10 entries from page 2800\n",
      "Scraping 10 entries from page 2810\n",
      "Scraping 10 entries from page 2820\n",
      "Scraping 10 entries from page 2830\n",
      "Scraping 10 entries from page 2840\n",
      "Scraping 10 entries from page 2850\n",
      "Scraping 10 entries from page 2860\n",
      "Scraping 10 entries from page 2870\n",
      "Scraping 10 entries from page 2880\n",
      "Scraping 10 entries from page 2890\n",
      "Scraping 10 entries from page 2900\n",
      "Scraping 10 entries from page 2910\n",
      "Scraping 10 entries from page 2920\n",
      "Scraping 10 entries from page 2930\n",
      "Scraping 10 entries from page 2940\n",
      "Scraping 10 entries from page 2950\n",
      "Scraping 10 entries from page 2960\n",
      "Scraping 10 entries from page 2970\n",
      "Scraping 10 entries from page 2980\n",
      "Scraping 10 entries from page 2990\n",
      "Scraping 10 entries from page 3000\n",
      "Scraping 10 entries from page 3010\n",
      "Scraping 10 entries from page 3020\n",
      "Scraping 10 entries from page 3030\n",
      "Scraping 10 entries from page 3040\n",
      "Scraping 10 entries from page 3050\n",
      "Scraping 10 entries from page 3060\n",
      "Scraping 10 entries from page 3070\n",
      "Scraping 10 entries from page 3080\n",
      "Scraping 10 entries from page 3090\n",
      "Scraping 10 entries from page 3100\n",
      "Scraping 10 entries from page 3110\n",
      "Scraping 10 entries from page 3120\n",
      "Scraping 10 entries from page 3130\n",
      "Scraping 10 entries from page 3140\n",
      "Scraping 10 entries from page 3150\n",
      "Scraping 10 entries from page 3160\n",
      "Scraping 10 entries from page 3170\n",
      "Scraping 10 entries from page 3180\n",
      "Scraping 10 entries from page 3190\n",
      "Scraping 10 entries from page 3200\n",
      "Scraping 2 entries from page 3210\n",
      "Finished scraping at 3220\n",
      "Writing 3212 entries to metadata.csv\n"
     ]
    }
   ],
   "source": [
    "%run scrape.py get-metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba5dc399",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run scrape.py download-decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8ad0b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: scikit-learn in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pandas in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from PyPDF2) (4.11.0)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Installing collected packages: PyPDF2, click, nltk\n",
      "Successfully installed PyPDF2-3.0.1 click-8.1.7 nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 nltk scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea07096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (4.43.3)\n",
      "Requirement already satisfied: torch in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from transformers) (0.24.3)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: colorama in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa3d7046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.7.5-cp39-cp39-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.10-cp39-cp39-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.8-cp39-cp39-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.9-cp39-cp39-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Using cached thinc-8.2.5-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.4.8-cp39-cp39-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Requirement already satisfied: jinja2 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy) (1.26.3)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached pydantic_core-2.20.1-cp39-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Using cached blis-0.7.11-cp39-cp39-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached marisa_trie-1.2.0-cp39-cp39-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached wrapt-1.16.0-cp39-cp39-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached spacy-3.7.5-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.8-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Using cached langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.10-cp39-cp39-win_amd64.whl (25 kB)\n",
      "Using cached preshed-3.0.9-cp39-cp39-win_amd64.whl (122 kB)\n",
      "Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Using cached pydantic_core-2.20.1-cp39-none-win_amd64.whl (1.9 MB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.4.8-cp39-cp39-win_amd64.whl (483 kB)\n",
      "Using cached thinc-8.2.5-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached blis-0.7.11-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Using cached cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Using cached marisa_trie-1.2.0-cp39-cp39-win_amd64.whl (152 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached wrapt-1.16.0-cp39-cp39-win_amd64.whl (37 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.8.2 pydantic-core-2.20.1 rich-13.7.1 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.3 wasabi-1.1.3 weasel-0.4.1 wrapt-1.16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.17.0 requires absl-py>=1.0.0, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires astunparse>=1.6.0, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires flatbuffers>=24.3.25, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires google-pasta>=0.1.1, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires grpcio<2.0,>=1.24.3, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires h5py>=3.10.0, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires keras>=3.2.0, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires libclang>=13.0.0, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires ml-dtypes<0.5.0,>=0.3.1, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires opt-einsum>=2.3.2, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires tensorboard<2.18,>=2.17, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires tensorflow-io-gcs-filesystem>=0.23.1; python_version < \"3.12\", which is not installed.\n",
      "tensorflow-intel 2.17.0 requires termcolor>=1.1.0, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 487.6 kB/s eta 0:00:27\n",
      "      --------------------------------------- 0.3/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.1/12.8 MB 6.5 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.1/12.8 MB 9.7 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.0/12.8 MB 11.3 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 3.9/12.8 MB 12.4 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.8/12.8 MB 13.2 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 13.9 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.5/12.8 MB 14.4 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 14.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.3/12.8 MB 15.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.0/12.8 MB 15.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 15.1 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.3/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.8/12.8 MB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.3/12.8 MB 17.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 11.8/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.3/12.8 MB 15.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 15.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 15.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 13.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.3)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.15.1)\n",
      "Requirement already satisfied: wrapt in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\py\\anaconda3\\envs\\newenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc71414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a2c317",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\xinyi\n",
      "[nltk_data]     Luo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\xinyi\n",
      "[nltk_data]     Luo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\xinyi\n",
      "[nltk_data]     Luo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the necessary NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a892a90",
   "metadata": {},
   "source": [
    "## Data-Preprocessing\n",
    "### Define the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62197450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_file):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts all text from a PDF file.\n",
    "    \"\"\"  \n",
    "    with open(pdf_file, \"rb\") as f:\n",
    "        pdf_reader = PdfReader(f)\n",
    "        text = []\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text.append(page.extract_text())\n",
    "        return \"\\n\".join(text)\n",
    "\n",
    "def convert_pdfs_to_texts(pdf_dir, txt_dir):\n",
    "    \"\"\"\n",
    "    Converts all PDF files in a specified directory to text files,\n",
    "    Text files are created in the txt_dir with the same name as the original\n",
    "    PDF files, but with a '.txt' extension.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(txt_dir):\n",
    "        os.makedirs(txt_dir)\n",
    "    \n",
    "    for pdf_file in os.listdir(pdf_dir):\n",
    "        if pdf_file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            \n",
    "            txt_file = pdf_file.replace('.pdf', '.txt')\n",
    "            txt_path = os.path.join(txt_dir, txt_file)\n",
    "            with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "                \n",
    "            #print(f\"Converted {pdf_file} to {txt_file}\")\n",
    "\n",
    "#pdf_dir = \"C:\\\\Users\\\\xinyi Luo\\\\Desktop\\\\Proj02\\\\decisions_4_6\"  \n",
    "#txt_dir = \"C:\\\\Users\\\\xinyi Luo\\\\Desktop\\\\Proj02\\\\texts_4_6\"  \n",
    "#convert_pdfs_to_texts(pdf_dir, txt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411596f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to split text into sections\n",
    "def split_text_into_sections(text):\n",
    "    \"\"\"\n",
    "    Splits a given text into predefined sections based on section titles.\n",
    "    Returns dictionary with KEY as section title and VALUES as the content accumulated under each section.\n",
    "    \"\"\"\n",
    "    sections = {\n",
    "        \"The complaint\": \"\",\n",
    "        \"What happened\": \"\",\n",
    "        \"What Ive decided  and why\": \"\",\n",
    "        \"My final decision\": \"\"\n",
    "    }\n",
    "\n",
    "    current_section = None\n",
    "    for line in text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Check if the line matches any section title\n",
    "        if \"The complaint\" in line:\n",
    "            current_section = \"The complaint\"\n",
    "        elif \"What happened\" in line:\n",
    "            current_section = \"What happened\"\n",
    "        elif \"What Ive decided  and why\" in line:\n",
    "            current_section = \"What Ive decided  and why\"\n",
    "        elif \"My final decision\" in line:\n",
    "            current_section = \"My final decision\"\n",
    "\n",
    "        if current_section:\n",
    "            sections[current_section] += line + \" \"\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c96f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing with spaCy\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a list of custom stop words\n",
    "custom_stop_words = ['mr', 'mrs', 'ms', 'miss', 'complaint', 'policy', 'insurance', \n",
    "                     'investigator', 'happen', 'decision', 'say', 'think', 'happen', \n",
    "                     'provide', 'consider', 'tell', 'explain', 'accept', 'include', \n",
    "                     'receive', 'decide', 'need', 'refer', 'not', 'take', 'bring', \n",
    "                     'cover', 'uphold', 'ask']\n",
    "\n",
    "# Add the custom stop words to the spaCy stop words set.\n",
    "for word in custom_stop_words:\n",
    "    nlp.vocab[word].is_stop = True\n",
    "\n",
    "def preprocess_with_spacy(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by cleaning and tokenizing it using spaCy.\n",
    "    Returns A tuple containing the list of processed tokens, the cleaned text, and the doc object.\n",
    "    \"\"\"\n",
    "    # Remove HTML tags.\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Process contractions and remove non-alphabet characters, except spaces.\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", '', text)\n",
    "    \n",
    "    # Remove excessive whitespace.\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Process the cleaned text with spaCy.\n",
    "    doc = nlp(cleaned_text.lower())\n",
    "    \n",
    "    # Remove stop words and punctuation, and perform lemmatization.\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    return tokens, cleaned_text, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db52ccb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final tokens(Sample): (['g', 'complain', 'fairmead', 'limited', 'failure', 'settle', 'claim', 'home', 'damage', 'property', 'own', 'rent'], 'Miss G has complained about Fairmead Insurance Limiteds failure to settle her claim under her Home Insurance policy for damage to a property she owns and rents out', miss g has complained about fairmead insurance limiteds failure to settle her claim under her home insurance policy for damage to a property she owns and rents out)\n"
     ]
    }
   ],
   "source": [
    "# Test an example\n",
    "text_sample = \"\"\"Miss G has complained about Fairmead Insurance Limiteds failure to settle her claim under \n",
    "her Home Insurance policy for damage to a property she owns and rents out. \n",
    "\"\"\"\n",
    "final_tokens_sample = preprocess_with_spacy(text_sample)\n",
    "print(\"Final tokens(Sample):\", final_tokens_sample[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9620319",
   "metadata": {},
   "source": [
    "## Extracting the category name of the insurance product object\n",
    "### Based on the paragraph \"The complaint\" (+ paragraph \"What happend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c822722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of complaint products by category for the years 2023-2024 which downloaded from FOS office website.\n",
    "# csv_file_path = 'C:/Users/xinyi Luo/Desktop/Proj02/23_24_insurance_list.csv'\n",
    "df_23_24_insurance_list = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Extract a list of unique product(group) names, removing any missing values.\n",
    "product_names = df_23_24_insurance_list['Product'].dropna().unique().tolist()\n",
    "product_group_names = df_23_24_insurance_list['Product group'].dropna().unique().tolist()\n",
    "\n",
    "# Create a dictionary mapping from product names to their corresponding product group names.\n",
    "product_to_group = dict(zip(df_23_24_insurance_list['Product'], df_23_24_insurance_list['Product group']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658499d",
   "metadata": {},
   "source": [
    "### Vectorise text: Convert text and product names to vectors using TF-IDF vectorisation.\n",
    "### Calculate cosine similarity: Calculate the cosine similarity between each product name and the text.\n",
    "### Select the product with highest similarity: Select the product name with the highest similarity (TOP<=3 if it exceeds the threshold) as the matching result.\n",
    "### Map product names to product groups, count and select the most frequently occurring category (product group) as the matching tag of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ec915d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a8f92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer().fit(product_names)\n",
    "product_vectors = vectorizer.transform(product_names)\n",
    "\n",
    "# Converts a text string into a TF-IDF vector\n",
    "def vectorize_text(text, vectorizer):\n",
    "    return vectorizer.transform([text])\n",
    "\n",
    "# Computes the cosine similarity between a vectorized text and a set of product names\n",
    "def compute_cosine_similarity(text_vector, product_vectors):\n",
    "    return cosine_similarity(text_vector, product_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea0a94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifies the top product matches based on a cosine similarity threshold\n",
    "def get_top_matches(similarities, product_names, threshold):\n",
    "    top_indices = np.where(similarities[0] >= threshold)[0]\n",
    "    return [(product_names[i], similarities[0][i]) for i in top_indices]\n",
    "\n",
    "def cosine_similarity_match(text, product_names, vectorizer, product_vectors, threshold=0.75):\n",
    "    tokens, cleaned_text, doc = preprocess_with_spacy(text)\n",
    "    text_vector = vectorize_text(cleaned_text, vectorizer)\n",
    "    similarities = compute_cosine_similarity(text_vector, product_vectors)\n",
    "    # Get the top matches based on the similarity scores and the threshold\n",
    "    reliable_matches = get_top_matches(similarities, product_names, threshold)\n",
    "    return reliable_matches if reliable_matches else []\n",
    "\n",
    "# Maps a list of product matches to their respective product groups and counts occurrences\n",
    "def map_to_product_group(matches, product_to_group):\n",
    "    if not matches:\n",
    "        return [], []\n",
    "    # Retrieve the product group for each match\n",
    "    groups = [product_to_group[match[0]] for match in matches if match[0] in product_to_group]\n",
    "    group_counter = Counter(groups)\n",
    "    most_common_group = group_counter.most_common(1)\n",
    "    return groups, most_common_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81cd31eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Product Names: [('Whole of life Assurance (Reviewable - Life Only)\\t', 0.8072137797448805), ('Whole of Life Assurance (Reviewable)\\t', 0.7406450917983347), ('Whole of Life Assurance (Non-reviewable)\\t', 0.6626413355644113)]\n",
      "Most Common Product Group: [('Life and Critical Illness Cover\\t', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Test example\n",
    "text = \"Mr C complains that Aviva Life & Pensions UK Limited has failed to provide a correct valuation for his whole of life policy, which he has held since 1974.\"\n",
    "\n",
    "matches = cosine_similarity_match(text, product_names, vectorizer, product_vectors)\n",
    "\n",
    "matched_groups, most_common_group = map_to_product_group(matches, product_to_group)\n",
    "\n",
    "print(\"Matched Product Names:\", matches)\n",
    "print(\"Most Common Product Group:\", most_common_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15006425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_texts_in_batches_cos(txt_dir, sections_to_process, batch_size=100, threshold=0.7):\n",
    "     \"\"\"\n",
    "    Processes text files in batches, extracts specific sections, matches them to products\n",
    "    using cosine similarity, and compiles results into a DataFrame.\n",
    "    \"\"\"\n",
    "    # Get all files those end with '.txt'\n",
    "    all_files = [os.path.join(txt_dir, file) for file in os.listdir(txt_dir) if file.endswith('.txt')]\n",
    "    total_files = len(all_files)\n",
    "    processed_data = []\n",
    "\n",
    "    # Process files in batches\n",
    "    for i in range(0, total_files, batch_size):\n",
    "        batch_files = all_files[i:i + batch_size]\n",
    "        for txt_file in batch_files:\n",
    "            with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            sections = split_text_into_sections(text)\n",
    "            processed_sections = {}\n",
    "            for section in sections_to_process:\n",
    "                content = sections.get(section, \"\")\n",
    "                matches = cosine_similarity_match(content, product_names, vectorizer, product_vectors, threshold)\n",
    "                processed_sections[f\"{section}_matches\"] = matches\n",
    "                \n",
    "                # Find top match if matches exist and are above threshold\n",
    "                if matches:\n",
    "                    matches.sort(key=lambda x: x[1], reverse=True)\n",
    "                    top_match = matches[0] if matches[0][1] >= threshold else None\n",
    "                    matched_groups, most_common_group = map_to_product_group(matches, product_to_group)\n",
    "                    processed_sections[f\"{section}_most_common_group\"] = most_common_group\n",
    "                else:\n",
    "                    top_match = None\n",
    "                    most_common_group = None\n",
    "                \n",
    "                # Store top match and most common group name, cleaning \"\\t\" for subsequent analysis\n",
    "                processed_sections[f\"{section}_top_match\"] = top_match[0] if top_match else None\n",
    "                most_common_group_name = most_common_group[0][0].replace(\"\\t\", \"\") if most_common_group else None\n",
    "                processed_sections[f\"{section}_most_common_group_name\"] = most_common_group_name\n",
    "                \n",
    "            processed_data.append({\"file_name\": os.path.basename(txt_file), **processed_sections})\n",
    "        \n",
    "    df = pd.DataFrame(processed_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c59ec77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the batch processing function\n",
    "txt_dir = \"C:\\\\Users\\\\xinyi Luo\\\\Desktop\\\\Proj02\\\\test_sample\"\n",
    "sections_to_process = [\"The complaint\"]\n",
    "df = process_texts_in_batches_cos(txt_dir, sections_to_process, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c58bc725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>The complaint_matches</th>\n",
       "      <th>The complaint_most_common_group</th>\n",
       "      <th>The complaint_top_match</th>\n",
       "      <th>The complaint_most_common_group_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DRN-4020987.txt</td>\n",
       "      <td>[(Whole of life Assurance (Reviewable - Life O...</td>\n",
       "      <td>[(Life and Critical Illness Cover\\t, 2)]</td>\n",
       "      <td>Whole of life Assurance (Reviewable - Life Onl...</td>\n",
       "      <td>Life and Critical Illness Cover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRN-4067234.txt</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DRN-4123997.txt</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DRN-4130617.txt</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DRN-4140715.txt</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         file_name                              The complaint_matches  \\\n",
       "0  DRN-4020987.txt  [(Whole of life Assurance (Reviewable - Life O...   \n",
       "1  DRN-4067234.txt                                                 []   \n",
       "2  DRN-4123997.txt                                                 []   \n",
       "3  DRN-4130617.txt                                                 []   \n",
       "4  DRN-4140715.txt                                                 []   \n",
       "\n",
       "            The complaint_most_common_group  \\\n",
       "0  [(Life and Critical Illness Cover\\t, 2)]   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "\n",
       "                             The complaint_top_match  \\\n",
       "0  Whole of life Assurance (Reviewable - Life Onl...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "  The complaint_most_common_group_name  \n",
       "0      Life and Critical Illness Cover  \n",
       "1                                 None  \n",
       "2                                 None  \n",
       "3                                 None  \n",
       "4                                 None  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0beeb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered results have been saved.\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame for rows where either 'The complaint_top_match' or 'The complaint_most_common_group_name' is not null.\n",
    "filtered_df = df[(df['The complaint_top_match'].notna()) | (df['The complaint_most_common_group_name'].notna())]\n",
    "\n",
    "if not filtered_df.empty:\n",
    "    filtered_df.to_csv(\"filtered_results_4_6_cosin_70.csv\", index=False)\n",
    "    print(\"Filtered results have been saved.\")\n",
    "else:\n",
    "    print(\"No non-empty rows to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
